# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g9v1yWeZ-QgNmS2AhKwNt4wsgAxoG85s
"""

# imports
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from sklearn.tree import DecisionTreeClassifier ,plot_tree, export_text


from sklearn.linear_model import RidgeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV,KFold
from sklearn.pipeline import Pipeline


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from sklearn.metrics import roc_curve, auc

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

df = pd.read_csv('LoanStats_securev1.csv')

# data exploration
print("Dataset shape: ", df.shape)
print("Column names: ", df.columns.tolist)
print("\mFirts 5 rows: ")
print(df.head())
print("\nData types: ")
print(df.dtypes)

# (checking for missing values)
print("Missing values per column: ")
print(df.isnull().sum())

# dropping completely empty columns
df = df.dropna(axis=1, how='all')
#removing missing columns
columns_to_drop = ['member_id', 'desc', 'url', 'deferral_term', 'hardship_amount'
, 'hardship_leangth', 'hardship_dpd', 'orig_projected_additional_accrued_interest'
, 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'settlement_amount'
, 'settlement_percentage', 'settlement_term', 'dept_settlement_flag_date'
,'settlement_date', 'next_pymnt_d']
#dropping columns with many missing values (over 80%)
missing_percent = df.isnull().mean()
highly_missing_columns = missing_percent[missing_percent > 0.8].index.tolist()
columns_to_drop.extend(highly_missing_columns)

df.drop(columns = columns_to_drop, errors = 'ignore', inplace=True)

#checking dataset shape after column deletion
print(f"Rows: {len(df)}")
print(f"Columns: {len(df.columns)}" )
print("Shape after dropping high-missing columns:", df.shape)

# filling missing values (numerical with median, categorial with mode)

#identifying categorial and numerical columns
numerical_columns  = df.select_dtypes(include=[np.number]).columns
categorial_columns = df.select_dtypes(include=['object']).columns

# filling numerical values
if len(numerical_columns) > 0:
    numerical_imputer = SimpleImputer(strategy='median')
    df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])

# filling categorial columns
if len(categorial_columns) > 0:
  categorial_imputer = SimpleImputer(strategy='most_frequent')
  df[categorial_columns] = categorial_imputer.fit_transform(df[categorial_columns])

print('Missing values after automatic filling:')
print(df.isnull().sum())

# taking a sample
SAMPLE_SIZE = 55000
df_sample = df.sample(n= min(SAMPLE_SIZE, len(df)), random_state=42)

# detecting outliers with IQR

# columns with outliers

for column in df_sample.columns:
  if df_sample[column].dtype in ['int64', 'float64']:
    Q1, Q3 = df_sample[column].quantile([0.25, 0.75])
    IQR = Q3 - Q1
    outliers = ((df_sample[column] < (Q1 - 1.5 * IQR)) | (df_sample[column] > (Q3 + 1.5*IQR))).sum()

    if outliers > 0 :
      print(f"{column}: {outliers} outliers")

# handling outliers
#capping all outliers
for column in df_sample.columns:
  if df_sample[column].dtype in ['int64', 'float64']:
    Q1, Q3 = df_sample[column].quantile([0.25, 0.75])
    IQR = Q3 - Q1
    upper_bound = Q3 + 1.5 * IQR
    lower_bound = Q3 - 1.5 * IQR
    df_sample[column] = np.where(df_sample[column]< lower_bound, lower_bound, df_sample[column])
    df_sample[column] = np.where(df_sample[column]> upper_bound, upper_bound, df_sample[column])

print('Outliers after capping:')
print(df_sample.describe())

# encoding
print(f"Encoding categorial columns and there are {len(categorial_columns)} categorial columns")
for column in categorial_columns:
  le = LabelEncoder()
  df_sample[column] =  le.fit_transform(le.fit_transform(df_sample[column]))

  print(f"\n{column} mapping:")
  mapping = dict(zip(le.classes_, le.transform(le.classes_)))
  print(mapping)

# normalization with standard scaler

scaler = StandardScaler()
scaled_data = pd.DataFrame(scaler.fit_transform(df_sample), columns = df_sample.columns)
print(f"Dataset shape after normalization: {scaled_data.shape}")

# before applying PCA,checking what the data looks like with heatmap
sns.heatmap(scaled_data.corr())

# dimensionality reduction(PCA)
pca = PCA(n_components=0.95)
pca_data = pca.fit_transform(scaled_data)
pca_df = pd.DataFrame(pca_data)

#clean data frame
pca_df = pd.DataFrame(pca_data, columns = [f'PC{i+1}' for i in range(pca_data.shape[1])])

print(f"Dataset feature before PCA: {scaled_data.shape[1]}")
print(f"Dataset components after PCA: {pca_df.shape[1]}")
print(f"Kept variance: {pca.explained_variance_ratio_.sum()*100:.1f}%")
print(f"New shape: {pca_df.shape}")

# define x and y which can identify good and bad

df_raw = pd.read_csv("LoanStats_securev1.csv", low_memory=False)

good_statuses = ["Fully Paid", "Current", "In Grace Period"]

df_sup = df_sample.copy()
raw_status = df_raw.loc[df_sup.index, "loan_status"]

mask = raw_status.notna()
df_sup = df_sup.loc[mask].copy()
raw_status = raw_status.loc[mask]

df_sup["loan_good"] = raw_status.isin(good_statuses).astype(int)

explicit_leak_cols = ["loan_good", "loan_status", "id", "member_id"]

leakage_keywords = [
    "pymnt",
    "out_prncp",
    "recover",
    "total_rec",
    "total_pymnt",
    "settlement",
    "hardship",
    "last_fico",
    "last_credit_pull",
]

auto_leak_cols = [c for c in df_sup.columns if any(k in c for k in leakage_keywords)]
cols_to_drop = list(set(explicit_leak_cols + auto_leak_cols))

X = df_sup.drop(columns=cols_to_drop, errors="ignore")
y = df_sup["loan_good"]

# train and test the data

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

#  XGBClassifier model

model = XGBClassifier(
    n_estimators=400,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),
    eval_metric="logloss",
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)


#test Accuracy for model
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=["Bad", "Good"]))


cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(cm, display_labels=["Bad", "Good"])
disp.plot(cmap="Blues")
plt.show()

"""# Decision Tree Classifier"""

#Initialize the Decision Tree Classifier
dt = DecisionTreeClassifier(criterion='entropy',max_depth=5,min_samples_split=50,min_samples_leaf=20,
                            class_weight='balanced',random_state=42)
#Train (Fit) the Model
dt.fit(X_train, y_train)

# Export the Decision Tree rules as text
tree_rules = export_text(dt, feature_names=list(X.columns))
print(tree_rules)

# Decision Tree Classifier model accuracy and precision and recall
y_dt_proba=dt.predict(X_test)
prec_dt = precision_score(y_test, y_pred)
rec_dt = recall_score(y_test, y_pred)
f1_dt = f1_score(y_test, y_pred)
print(f"tree accuracy: {accuracy_score(y_test,y_dt_proba)}")
print(f"tree precision: {prec_dt}")
print(f"tree recall: {rec_dt}")

#ConfusionMatrix for Decision Tree
cm_dt = confusion_matrix(y_test, y_dt_proba, labels=[0, 1])
disp_dt = ConfusionMatrixDisplay(cm_dt, display_labels=["Bad", "Good"])
disp_dt.plot(values_format="d")
plt.title("Decision Tree ConfusionMatrix")
plt.show()

#the figure of desecion tree
plt.figure(figsize=(30, 10))
plot_tree(dt,feature_names=X_train.columns,class_names=['Bad', 'Good'],filled=True,max_depth=5,fontsize=8)
plt.title("Decision Tree")
plt.show()

"""# RidgeClassifier Model"""

# Create a pipeline: scale features, apply PCA, and train RidgeClassifier with balanced classes

steps =[("scaler", StandardScaler()), ("pca",PCA(n_components=0.95)), ("RidgeClassifier",RidgeClassifier(class_weight={1:1, 0:10}))]
pipeline = Pipeline(steps)

# apply GridSearchCV to find the best alpha

param_grid = {"RidgeClassifier__alpha": [0.1, 1.0, 10.0, 100.0, 1000.0]}
Ridge_Classifier_cv = GridSearchCV(pipeline, param_grid , cv=5)

# Train the RidgeClassifier

Ridge_Classifier_cv.fit(X_train, y_train)

# Get the best trained model, make predictions

best_ridge_model = Ridge_Classifier_cv.best_estimator_
y_pred_Ridge = best_ridge_model.predict(X_test)
best_alpha = best_ridge_model.named_steps['RidgeClassifier'].alpha

# print best alpha and accuracy

print(f"Best alpha: {best_alpha}")
print(f"Ridge accuracy: {accuracy_score(y_test,y_pred_Ridge )}")

# Plot the confusion matrix for RidgeClassifier predictions

plt.figure(figsize=(9,7))
cm_Ridge = confusion_matrix(y_test, y_pred_Ridge, labels=[0, 1])
disp = ConfusionMatrixDisplay(cm_Ridge, display_labels=["Bad", "Good"])
disp.plot(cmap="viridis", values_format="d")
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title("Supervised Model - Ridge Classifier" ,  fontsize=13, fontweight="bold")
plt.xlabel("Predicted Label", fontsize=12)
plt.ylabel("True Label", fontsize=12)
plt.show()

"""#Data Before (K-Means) Model"""

plt.figure(figsize=(8,6))
plt.scatter(pca_df["PC1"], pca_df["PC2"], s=10, alpha=0.5)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("Scatter Plot of PCA-transformed Data (Before K-Means)")
plt.show()

"""**Elbow Method **For Determine Correct Nomber 'K'"""

SSE=[]
K = range(1, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_df)
    SSE.append(kmeans.inertia_)

plt.plot(K, SSE, 'o-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

"""**Create K-MEans Model **"""

kmeans = KMeans(n_clusters=7, random_state=42)
clusters = kmeans.fit_predict(pca_df)

pca_df["cluster"] = clusters
pca_df

"""(Cluster vs loan_good)"""

pd.crosstab(df_sup["loan_good"], pca_df["cluster"])

"""PCA Scatter Plot with Clusters"""

plt.scatter(pca_df["PC1"], pca_df["PC2"], c=pca_df["cluster"], cmap='viridis', s=20)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("K-Means Clustering Visualization")
plt.show()

"""Silhouette Score"""

score = silhouette_score(pca_df.drop("cluster", axis=1), pca_df["cluster"])
print("Silhouette Score:", score)

"""# results and evaluation"""

#test shapes
print("X_train:", X_train.shape)
print("X_test :", X_test.shape)
print("y_train:", y_train.shape)
print("y_test :", y_test.shape)

# function for evaluating each model
def evaluate_model(model, X_train, y_train, X_test, y_test, model_name="Model"):

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

   #auc for roc curve
    auc = None
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test,y_proba)
    elif hasattr(model, "decision_function"):
        y_proba = model.decision_function(X_test)
        auc = roc_auc_score(y_test,y_proba)



    return {
        "Model": model_name,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1,
        "AUC":auc   }

#apeend all results for models
results = []


#Random Forest
results.append(evaluate_model(model, X_train, y_train, X_test, y_test, "XGBClassifier"))
#decision tree
results.append(evaluate_model(dt, X_train, y_train, X_test, y_test, model_name="DecisionÂ Tree"))
#ridge classifier
results.append(evaluate_model(best_ridge_model, X_train, y_train, X_test, y_test, model_name="ridge classifier"))

results_df=pd.DataFrame(results)
results_df

#roc curve for models

# XGBClassifier
xg_proba = model.predict_proba(X_test)[:, 1]
xg_fpr, xg_tpr, _ = roc_curve(y_test, xg_proba)
xg_auc = auc(xg_fpr, xg_tpr)

# Ridge Classifier
y_proba_ridge=best_ridge_model.decision_function(X_test)
ridge_fpr, ridge_tpr, _ = roc_curve(y_test, y_proba_ridge)
ridge_auc = auc(ridge_fpr, ridge_tpr)

# Decision Tree
dt_proba = dt.predict_proba(X_test)[:, 1]
dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_proba)
dt_auc = auc(dt_fpr, dt_tpr)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(xg_fpr, xg_tpr, label=f"XGBClassifier (AUC = {xg_auc:.3f})")
plt.plot(ridge_fpr, ridge_tpr, label=f"Ridge Classifier (AUC = {ridge_auc:.3f})")
plt.plot(dt_fpr, dt_tpr, label=f"Decision Tree (AUC = {dt_auc:.3f})")


plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Comparison for models")
plt.legend()
plt.grid()
plt.show()